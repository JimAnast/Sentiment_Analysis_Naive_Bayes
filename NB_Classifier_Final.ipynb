{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveBayes_Classifier Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from codecs import open\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Iterable\n",
    "import math\n",
    "\n",
    "def read_documents(doc_file):\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs, all_labels = read_documents('all_sentiment_shuffled.txt')\n",
    "\n",
    "split_point = int(0.80*len(all_docs))\n",
    "train_docs = all_docs[:split_point]\n",
    "train_labels = all_labels[:split_point]  \n",
    "val_docs = all_docs[split_point:]\n",
    "val_labels = all_labels[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()               #Create the vocabulary of all unique words\n",
    "for review in train_docs:\n",
    "    for word in review:\n",
    "        vocabulary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb(train_docs, train_labels, vocabulary, alpha = 1): # If non given, alpha is 1. Alpha is the smoothing parameter we are going to use \n",
    "    \n",
    "    word_appear_in_pos_neg_review = {word: [0]*2 for word in vocabulary} #every word has two indices, one for negative and one for positve appearances \n",
    "\n",
    "    # we will use index 0 for negative and 1 for positive reviews \n",
    "    S_Neg_Pos = [0,0]            # List with the Sums of all words contained in negative and positive labeled reviews\n",
    "    \n",
    "    for i in range(len(train_docs)):\n",
    "        if train_labels[i] == 'neg':\n",
    "            for word in train_docs[i]:\n",
    "                word_appear_in_pos_neg_review[word][0] += 1\n",
    "                S_Neg_Pos[0] += 1\n",
    "        else:\n",
    "            for word in train_docs[i]:\n",
    "                word_appear_in_pos_neg_review[word][1] += 1\n",
    "                S_Neg_Pos[1] += 1\n",
    "                \n",
    "    likelihood_dict = {word: [0]*2 for word in vocabulary}   # Dictionary for the likelihood of words being in positive or negative reviews \n",
    "     \n",
    "    for word in vocabulary:\n",
    "        likelihood_dict[word][0] = (word_appear_in_pos_neg_review[word][0] + alpha)/ (S_Neg_Pos[0] + alpha*len(vocabulary))\n",
    "        likelihood_dict[word][1] = (word_appear_in_pos_neg_review[word][1] + alpha)/ (S_Neg_Pos[1] + alpha*len(vocabulary))\n",
    "        \n",
    "    total_neg_reviews = 0    #Sum of all negative reviews\n",
    "    total_pos_reviews = 0    #Sum of all positive reviews\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i] == 'neg':\n",
    "            total_neg_reviews += 1\n",
    "        else:\n",
    "            total_pos_reviews += 1      \n",
    "        \n",
    "    prior_probs = [0,0]                           # We estimate the prior probabilities of negative and positive reviews\n",
    "    prior_probs[0] = total_neg_reviews/len(train_docs)\n",
    "    prior_probs[1] = total_pos_reviews/len(train_docs)     \n",
    "    \n",
    "    return likelihood_dict, prior_probs, S_Neg_Pos\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_di, prior_probab, S_Neg_Pos = train_nb(train_docs, train_labels, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_doc_label(document, likelihood_dict, prior_probab, vocabulary):\n",
    "    score_prob = [0,0]\n",
    "    score_prob[0] = prior_probab[0]\n",
    "    score_prob[1] = prior_probab[1]\n",
    "    for word in document:\n",
    "        if word in vocabulary:\n",
    "            if (score_prob[0]>= 10**(-305)) and (score_prob[1]>= 10**(-305)):  # We put that limitation here, since python \n",
    "                score_prob[0] *= likelihood_dict[word][0]                      # cannot handle very very small numbers and converts them  \n",
    "                score_prob[1] *= likelihood_dict[word][1]                      # into minus infinity instead.\n",
    "            \n",
    "        # We chose to ignore the words that are not included in the vocabulary, since that gave us better accuracy results\n",
    "        # Otherwise, we would include them with the following way\n",
    "        \n",
    "        #else:\n",
    "            #score_prob[0] *= alpha /(S_Neg_Pos[0]+alpha*len(vocabulary))    # We would have to include alpha in the parameters of   \n",
    "            #score_prob[1] *= alpha /(S_Neg_Pos[1]+alpha*len(vocabulary))    # the function if we run this else condition\n",
    "        # print (score_prob)\n",
    "         \n",
    "    score_prob = np.log(score_prob)    #Converting the probability into log probability\n",
    "    return score_prob  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['a', 'top-quality', 'performance']   # We can put any review to test our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = score_doc_label(doc, likelihood_di, prior_probab, vocabulary)  # Run this to compute the score that we need for the \n",
    "                                                                       # classification below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-13.48689174 -12.80785836]\n"
     ]
    }
   ],
   "source": [
    "print(score) # We get the log probability array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.38904828e-06, 2.73916245e-06])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_score = np.exp(score)   # Run this if you want to convert the log probability that appeared above to a normal probability array\n",
    "new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_nb(document, score):\n",
    "    if score[0] < score[1]:\n",
    "        guess = \"pos\"\n",
    "    else:\n",
    "        guess = \"neg\"\n",
    "    #else:\n",
    "        #guess = \"Cannot decide: Equally likely to be positive or negative review\"    # In the extreme case that positive and negative have the exact same score \n",
    "        \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = classify_nb(doc, score)\n",
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the classifier\n",
    "\n",
    "def classify_documents(docs, vocabulary, likelihood_di, prior_probab):  \n",
    "    \n",
    "    label_list = []\n",
    "    score_sentence = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        score = score_doc_label(doc, likelihood_di, prior_probab, vocabulary)\n",
    "        # print(score)\n",
    "        guess = classify_nb(doc, score)\n",
    "        label_list.append(guess)\n",
    "        score_sentence.append(score)\n",
    "    \n",
    "    return label_list, score_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_labels, score_sentence = classify_documents(val_docs, vocabulary, likelihood_di, prior_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_labels, guessed_labels):\n",
    "    correct_count = 0\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] == guessed_labels[i]:\n",
    "            correct_count += 1\n",
    "            \n",
    "    acc = correct_count/len(true_labels)     \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7960553923625682"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accur = accuracy(val_labels, guess_labels)\n",
    "accur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(val_labels, guess_labels):\n",
    "    true_pos = 0\n",
    "    fals_pos = 0\n",
    "    \n",
    "    for label_t, label_p in zip (val_labels,guess_labels):\n",
    "        if label_t == 'pos' and label_p == 'pos':\n",
    "            true_pos += 1\n",
    "        if label_t == 'neg' and label_p == 'pos':\n",
    "            fals_pos += 1\n",
    "    \n",
    "    total = true_pos + fals_pos\n",
    "    \n",
    "    precis = true_pos/total\n",
    "    \n",
    "    return precis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8018099547511313"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precis = precision(val_labels, guess_labels)\n",
    "precis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(val_labels, guess_labels):\n",
    "    true_pos = 0\n",
    "    fals_neg = 0\n",
    "    \n",
    "    for label_t, label_p in zip (val_labels,guess_labels):\n",
    "        if label_t == 'pos' and label_p == 'pos':\n",
    "            true_pos += 1\n",
    "        if label_t == 'pos' and label_p == 'neg':\n",
    "            fals_neg += 1\n",
    "    \n",
    "    total = true_pos + fals_neg\n",
    "    \n",
    "    recall = true_pos/total\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7684301821335646"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_ = recall(val_labels, guess_labels)\n",
    "recall_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(val_labels, guess_labels):\n",
    "    precis = precision(val_labels, guess_labels)\n",
    "    rec = recall(val_labels, guess_labels)\n",
    "    \n",
    "    f = 2 * (precis * rec) / (precis +rec)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7847652790079717"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_1 = f_score(val_labels, guess_labels)\n",
    "f_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors_indices(val_labels = val_labels, guess_labels = guess_labels):\n",
    "    false_predict = []     # List in which we keep all the indices of the misclassified documents\n",
    "    for i in range(len(val_labels)):\n",
    "        if (val_labels[i] == 'pos' and guess_labels[i] == 'neg') or (val_labels[i] == 'neg' and guess_labels[i] == 'pos'):\n",
    "            false_predict.append(i)\n",
    "    return false_predict                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred = find_errors_indices(val_labels, guess_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((val_docs, score_sentence, val_labels, guess_labels))\n",
    "df = df.T\n",
    "df = df.rename(columns = {0: \"Review\", 1: \"Neg_Pos\", 2: \"True_Label\", 3: \"Guess_Label\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Neg_Pos</th>\n",
       "      <th>True_Label</th>\n",
       "      <th>Guess_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[do, not, buy, this, iron, ., it, 's, fabulous...</td>\n",
       "      <td>[-476.1805840041536, -488.49501638443303]</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[the, series, just, keeps, on, getting, better...</td>\n",
       "      <td>[-478.8695832044195, -476.2881359649154]</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[i, bought, this, apple, humidifier, in, early...</td>\n",
       "      <td>[-686.6196141857182, -709.1696617728866]</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[i, highly, recommend, this, super, wide, angl...</td>\n",
       "      <td>[-364.6633195247102, -353.71965625805865]</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[this, is, one, of, the, better, historical, d...</td>\n",
       "      <td>[-706.3890416473079, -695.9936451532267]</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  [do, not, buy, this, iron, ., it, 's, fabulous...   \n",
       "1  [the, series, just, keeps, on, getting, better...   \n",
       "2  [i, bought, this, apple, humidifier, in, early...   \n",
       "3  [i, highly, recommend, this, super, wide, angl...   \n",
       "4  [this, is, one, of, the, better, historical, d...   \n",
       "\n",
       "                                     Neg_Pos True_Label Guess_Label  \n",
       "0  [-476.1805840041536, -488.49501638443303]        neg         neg  \n",
       "1   [-478.8695832044195, -476.2881359649154]        pos         pos  \n",
       "2   [-686.6196141857182, -709.1696617728866]        neg         neg  \n",
       "3  [-364.6633195247102, -353.71965625805865]        pos         pos  \n",
       "4   [-706.3890416473079, -695.9936451532267]        pos         pos  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[((df['True_Label'] == 'neg') & (df['Guess_Label'] == 'pos')) | ((df['True_Label'] == 'pos') & (df['Guess_Label'] == 'neg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Neg_Pos</th>\n",
       "      <th>True_Label</th>\n",
       "      <th>Guess_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[i, agree, with, other, reviewers, that, it, f...</td>\n",
       "      <td>[-522.2997780517946, -517.1997485904006]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>[this, camera, has, a, very, poor, lens, ., at...</td>\n",
       "      <td>[-480.25153518481494, -476.1033272001385]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[this, book, offers, more, information, about,...</td>\n",
       "      <td>[-474.45296619422317, -473.1350092725393]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>[i, was, able, to, scout, out, the, different,...</td>\n",
       "      <td>[-329.82249598660104, -329.9073854379372]</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>[i, have, been, a, fan, since, valotte, ., and...</td>\n",
       "      <td>[-699.5516101932159, -703.9787347446481]</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2372</td>\n",
       "      <td>[the, text, is, ok, and, the, way, the, story,...</td>\n",
       "      <td>[-674.8046173025884, -672.2745429779512]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2376</td>\n",
       "      <td>[i, 'm, not, sure, what, compelled, me, to, se...</td>\n",
       "      <td>[-707.6027104635142, -704.8330676653848]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2377</td>\n",
       "      <td>[after, watching, \", harsh, times, \", we, wond...</td>\n",
       "      <td>[-705.381183513228, -703.0399534529643]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2378</td>\n",
       "      <td>[the, story, here, dose, n't, matter, ., the, ...</td>\n",
       "      <td>[-704.3883541325342, -702.7512472813743]</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2379</td>\n",
       "      <td>[i, liked, everything, about, this, product, e...</td>\n",
       "      <td>[-277.41584975751505, -278.82151918813116]</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review  \\\n",
       "9     [i, agree, with, other, reviewers, that, it, f...   \n",
       "10    [this, camera, has, a, very, poor, lens, ., at...   \n",
       "12    [this, book, offers, more, information, about,...   \n",
       "14    [i, was, able, to, scout, out, the, different,...   \n",
       "15    [i, have, been, a, fan, since, valotte, ., and...   \n",
       "...                                                 ...   \n",
       "2372  [the, text, is, ok, and, the, way, the, story,...   \n",
       "2376  [i, 'm, not, sure, what, compelled, me, to, se...   \n",
       "2377  [after, watching, \", harsh, times, \", we, wond...   \n",
       "2378  [the, story, here, dose, n't, matter, ., the, ...   \n",
       "2379  [i, liked, everything, about, this, product, e...   \n",
       "\n",
       "                                         Neg_Pos True_Label Guess_Label  \n",
       "9       [-522.2997780517946, -517.1997485904006]        neg         pos  \n",
       "10     [-480.25153518481494, -476.1033272001385]        neg         pos  \n",
       "12     [-474.45296619422317, -473.1350092725393]        neg         pos  \n",
       "14     [-329.82249598660104, -329.9073854379372]        pos         neg  \n",
       "15      [-699.5516101932159, -703.9787347446481]        pos         neg  \n",
       "...                                          ...        ...         ...  \n",
       "2372    [-674.8046173025884, -672.2745429779512]        neg         pos  \n",
       "2376    [-707.6027104635142, -704.8330676653848]        neg         pos  \n",
       "2377     [-705.381183513228, -703.0399534529643]        neg         pos  \n",
       "2378    [-704.3883541325342, -702.7512472813743]        neg         pos  \n",
       "2379  [-277.41584975751505, -278.82151918813116]        pos         neg  \n",
       "\n",
       "[486 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []                     # This list will contain the 10 most wrongly directed misclassified document indices\n",
    "for j in range(10):\n",
    "    max_diff = abs(df2['Neg_Pos'][9][0] - df2['Neg_Pos'][9][1])\n",
    "    index = 9\n",
    "    for i in false_pred:\n",
    "        if (abs(df2['Neg_Pos'][i][0] - df2['Neg_Pos'][i][1]) > max_diff) and (i not in index_list):\n",
    "            max_diff = abs(df2['Neg_Pos'][i][0] - df2['Neg_Pos'][i][1])\n",
    "            index = i\n",
    "    index_list.append(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1287, 173, 231, 1626, 1756, 2181, 121, 1551, 1856, 2254]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review         [i, 've, been, using, norton, antivirus, 2002,...\n",
       "Neg_Pos                 [-684.8728016026175, -706.3533809219927]\n",
       "True_Label                                                   pos\n",
       "Guess_Label                                                  neg\n",
       "Name: 1287, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[1287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.480579319375124\n"
     ]
    }
   ],
   "source": [
    "print(abs(df['Neg_Pos'][1287][0] - df['Neg_Pos'][1287][1]))    # This is the difference in the log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 've been using norton antivirus 2002 with annual updates of patches forever with norton internet security 2003. now that it 's time to renewal norton antivirus , i thought i switch to trend micro internet secuirty 2005 after all the glowing wins it received from certain mainstream computer magazines . i like norton 2002 but it was a memory resident hog in the taskmanager considering i was running a p3 500 laptop with 196mb on winxp pro , resources are valuable . also newer norton antivirus version were a hit and miss . after also reading reviews at amazon , i took the plunge with trend micro . my review of this product can be best summarize along with it 's relations to norton antivirus 2002 ( updated ) . pros of trend micro 2005 : 1. fastest virus scan i 've seen . i have not find any virus in my system yet whereas norton would fail in about .01% of the time ( i.e. one stalling virus every year ) . 2. check incoming and outgoing mail like norton 3. scan selected files as i chose just like norton 4. alert and block virus threats 5. monitor nearby wifi connection 6. $25 mail in rebate making the price net at $15. ( it 's been 2 months since i 've mailed it and not received it yet ) . 7. uses about 10mb of system background resources versus norton 's 30mb . cons against trend micro 2005 : 1. very annoying auto-virus update every day that i log on the internet . i only have a 56k connection and updating takes 5 minutes . i 'm not trying to save the internet but everyday update may be a bit too much so i would prefer to have a preference setting that i can choose myself . norton is not as annoying . 2. on a wifi network , trend micro firewall failed 5 of the gibson research shields up test . ports were open . norton internet firewall close all major holes except for the non-critical ones which was n't a big worry . i ended up disabling trend micro firewall and using the norton firewall instead as it 's a full software and much better analysis and deeper protection . 3. anti-spam and data protection were subpar as i got advertisements and pop-up from certain websites like west cost major newspaper . using norton firewall stops them dead ; never seen them again . 4. still waiting for rebate . overall , i think using trendmicro internet security 2005 for it 's fast virus check and a separate norton internet firewall is the ideal combo for me . on it 's own merit , trend micro would be perfect only if ram and other resources are low as the case with my laptop but i would n't mind using norton if i had plentiful more ram .\n"
     ]
    }
   ],
   "source": [
    "to_read = \" \".join(val_docs[1287])\n",
    "print(to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10    # We will implement the 10-fold Cross Validation, therefore we give the number of folds N=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "S_accur = 0                 # For calculating the accuracy later of cross validation below\n",
    "S_f_1 = 0                   # Same for F1 score\n",
    "\n",
    "for fold_nbr in range(N):\n",
    "    split_point_1 = int(float(fold_nbr)/N*len(all_docs))\n",
    "    split_point_2 = int(float(fold_nbr+1)/N*len(all_docs))\n",
    "\n",
    "    train_docs_fold = all_docs[:split_point_1] + all_docs[split_point_2:]\n",
    "    train_labels_fold = all_labels[:split_point_1] + all_labels[split_point_2:]\n",
    "    \n",
    "    new_vocabulary = set()               #Create the vocabulary of all unique words\n",
    "    for review in train_docs_fold:\n",
    "        for word in review:\n",
    "            new_vocabulary.add(word)\n",
    "    \n",
    "    val_docs_fold = all_docs[split_point_1:split_point_2]\n",
    "    val_labels_fold = all_labels[split_point_1:split_point_2]\n",
    "    \n",
    "    likelihood_di, prior_probab, S_Neg_Pos = train_nb(train_docs_fold, train_labels_fold, new_vocabulary)\n",
    "    guess_labels, score_sentence = classify_documents(val_docs_fold, new_vocabulary, likelihood_di, prior_probab)\n",
    "    \n",
    "    accur = accuracy(val_labels_fold, guess_labels)\n",
    "    S_accur += accur\n",
    "    \n",
    "    f_1 = f_score(val_labels_fold, guess_labels)\n",
    "    S_f_1 += f_1\n",
    "\n",
    "final_accur = S_accur / N    \n",
    "final_f_1 = S_f_1 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884844527468318"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_accur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833510236318384"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cross(true_labels, guessed_labels):   # for the  leave one out, as there is no index. \n",
    "    correct_count = 0\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels == guessed_labels:\n",
    "            correct_count += 1\n",
    "            \n",
    "    acc = correct_count/len(true_labels)     \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave-one-out Cross Validation\n",
    "\n",
    "S_accur = 0                 # Same as above, for metrics calculations\n",
    "S_f_1 = 0                   \n",
    "list_guesses = []           # We need these lists for the computation of the F1 score later on\n",
    "list_val_labels = []\n",
    "\n",
    "for i in range (100):\n",
    "    train_docs_fold = all_docs[:i] + all_docs[i+1:]\n",
    "    train_labels_fold = all_labels[:i] + all_labels[i+1:]\n",
    "    \n",
    "    new_vocabulary = set()               #Create the vocabulary of all unique words\n",
    "    for review in train_docs_fold:\n",
    "        for word in review:\n",
    "            new_vocabulary.add(word)\n",
    "    \n",
    "    val_docs_fold = all_docs[i]\n",
    "    val_labels_fold = all_labels[i]\n",
    "    list_val_labels.append(val_labels_fold)\n",
    "    \n",
    "    likelihood_di, prior_probab, S_Neg_Pos = train_nb(train_docs_fold, train_labels_fold, new_vocabulary)\n",
    "    guess_labels, score_sentence = classify_documents([val_docs_fold], new_vocabulary, likelihood_di, prior_probab)\n",
    "    list_guesses.append(guess_labels)\n",
    "    \n",
    "    accur = accuracy_cross([val_labels_fold], guess_labels)\n",
    "    S_accur += accur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.0\n"
     ]
    }
   ],
   "source": [
    "print(S_accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_accur = S_accur / 100\n",
    "final_accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we noticed that our list_guesses was nested. Thus giving errors. We fixed this by unflattening it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "            for x in flatten(item):\n",
    "                 yield x\n",
    "        else:        \n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_guesses = list(flatten(list_guesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108108108108109"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f_score(list_val_labels, list_guesses)\n",
    "f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
